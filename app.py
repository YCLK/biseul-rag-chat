import os
import gradio as gr
from langchain_classic.chains import RetrievalQA
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API")

# 2. ë¬¸ì„œ ë¡œë“œ
loaders = [
    TextLoader("school_regulations.txt", encoding="utf-8"),
    #CSVLoader("bad_rules.csv", encoding="cp949"),
    #CSVLoader("good_rules.csv", encoding="cp949"),
]

docs = []
for loader in loaders:
    docs.extend(loader.load())

# 3. ë¬¸ì„œ ë¶„í•  ë° ë²¡í„° ì €ì¥ì†Œ
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY))
retriever = vectorstore.as_retriever()

# 4. RAG ì²´ì¸ ìƒì„± (êµ¬ë²„ì „ ë°©ì‹: RetrievalQA)
llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=OPENAI_API_KEY)

# RetrievalQAëŠ” êµ¬ë²„ì „ì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ ì²´ì¸ì…ë‹ˆë‹¤.
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=False  # ë‹µë³€ë§Œ ë°›ê¸°
)

# 5. Gradio ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
def predict(message, history):
    # êµ¬ë²„ì „ì—ì„œëŠ” .invoke ëŒ€ì‹  .runì„ ì‚¬ìš©í•˜ê¸°ë„ í–ˆìœ¼ë‚˜, 
    # ìµœê·¼ ë²„ì „ í˜¸í™˜ì„ ìœ„í•´ invokeë¥¼ ì“°ë˜, ì•ˆë˜ë©´ .run(message)ë¡œ ë°”ê¾¸ì„¸ìš”.
    try:
        response = rag_chain.invoke(message)
        return response['result'] # RetrievalQAì˜ ê²°ê³¼ í‚¤ëŠ” ë³´í†µ 'result' ì…ë‹ˆë‹¤.
    except:
        return rag_chain.run(message) # ì•„ì£¼ êµ¬ë²„ì „ì¼ ê²½ìš° ëŒ€ë¹„

# 6. ì•± ì‹¤í–‰
if __name__ == "__main__":
    gr.ChatInterface(
        fn=predict,
        title="ğŸ« í•™êµ ìƒí™œê·œì • ì•ˆë‚´ ì±—ë´‡ (Legacy)",
        description="í•™êµ ê·œì¹™, ìƒì , ë²Œì ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.",
        examples=["ìš•ì„¤ì„ í•˜ë©´ ë²Œì ì´ ëª‡ ì ì´ì•¼?", "ì²­ì†Œë¥¼ ì˜í•˜ë©´ ìƒì ì„ ë°›ì„ ìˆ˜ ìˆì–´?"],
    ).launch()import os
import gradio as gr
from langchain_classic.chains import RetrievalQA
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API")

# 2. ë¬¸ì„œ ë¡œë“œ
loaders = [
    TextLoader("school_regulations.txt", encoding="utf-8"),
    #CSVLoader("bad_rules.csv", encoding="cp949"),
    #CSVLoader("good_rules.csv", encoding="cp949"),
]

docs = []
for loader in loaders:
    docs.extend(loader.load())

# 3. ë¬¸ì„œ ë¶„í•  ë° ë²¡í„° ì €ì¥ì†Œ
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings(api_key=OPENAI_API_KEY))
retriever = vectorstore.as_retriever()

# 4. RAG ì²´ì¸ ìƒì„± (êµ¬ë²„ì „ ë°©ì‹: RetrievalQA)
llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=OPENAI_API_KEY)

# RetrievalQAëŠ” êµ¬ë²„ì „ì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ ì²´ì¸ì…ë‹ˆë‹¤.
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=False  # ë‹µë³€ë§Œ ë°›ê¸°
)

# 5. Gradio ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
def predict(message, history):
    # êµ¬ë²„ì „ì—ì„œëŠ” .invoke ëŒ€ì‹  .runì„ ì‚¬ìš©í•˜ê¸°ë„ í–ˆìœ¼ë‚˜, 
    # ìµœê·¼ ë²„ì „ í˜¸í™˜ì„ ìœ„í•´ invokeë¥¼ ì“°ë˜, ì•ˆë˜ë©´ .run(message)ë¡œ ë°”ê¾¸ì„¸ìš”.
    try:
        response = rag_chain.invoke(message)
        return response['result'] # RetrievalQAì˜ ê²°ê³¼ í‚¤ëŠ” ë³´í†µ 'result' ì…ë‹ˆë‹¤.
    except:
        return rag_chain.run(message) # ì•„ì£¼ êµ¬ë²„ì „ì¼ ê²½ìš° ëŒ€ë¹„

# 6. ì•± ì‹¤í–‰
if __name__ == "__main__":
    gr.ChatInterface(
        fn=predict,
        title="ğŸ« í•™êµ ìƒí™œê·œì • ì•ˆë‚´ ì±—ë´‡ (Legacy)",
        description="í•™êµ ê·œì¹™, ìƒì , ë²Œì ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.",
        examples=["ìš•ì„¤ì„ í•˜ë©´ ë²Œì ì´ ëª‡ ì ì´ì•¼?", "ì²­ì†Œë¥¼ ì˜í•˜ë©´ ìƒì ì„ ë°›ì„ ìˆ˜ ìˆì–´?"],
    ).launch()
